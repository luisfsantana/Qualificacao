%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for Luis Sant Ana at 2013-09-12 11:45:28 -0300 


%% Saved with string encoding Unicode (UTF-8) 



@inproceedings{Anthill,
	Author = {Teodoro, G. and Sachetto, R. and Sertel, O. and Gurcan, M.N. and Meira, W. and Catalyurek, U. and Ferreira, R.},
	Booktitle = {Cluster Computing and Workshops, 2009. CLUSTER '09. IEEE International Conference on},
	Date-Added = {2013-09-12 14:44:03 +0000},
	Date-Modified = {2013-09-12 14:44:03 +0000},
	Doi = {10.1109/CLUSTR.2009.5289193},
	Issn = {1552-5244},
	Keywords = {coprocessors;digital filters;medical image processing;microcomputers;tumours;Anthill runtime environment;compute intensive applications;distributed execution;dual-core machine;event-driven filters;graphics processing unit;histopathology application;image analysis techniques;multi-core CPUs;neuroblastoma prognosis;octa-core machine;parallel co-processors;standalone execution;Biomedical computing;Cancer;Computer applications;Concurrent computing;Image analysis;Microscopy;Neoplasms;Niobium;Performance analysis;Runtime environment},
	Pages = {1-10},
	Title = {Coordinating the use of GPU and CPU for improving performance of compute intensive applications},
	Year = {2009},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/CLUSTR.2009.5289193}}

@incollection{StarSs,
	Author = {Ayguade, Eduard and Badia, RosaM. and Cabrera, Daniel and Duran, Alejandro and Gonzalez, Marc and Igual, Francisco and Jimenez, Daniel and Labarta, Jesus and Martorell, Xavier and Mayo, Rafael and Perez, JosepM. and Quintana-Ort{\'\i}, EnriqueS.},
	Booktitle = {Evolving OpenMP in an Age of Extreme Parallelism},
	Date-Added = {2013-09-12 14:41:59 +0000},
	Date-Modified = {2013-09-12 14:41:59 +0000},
	Doi = {10.1007/978-3-642-02303-3_13},
	Editor = {M{\"u}ller, MatthiasS. and Supinski, BronisR. and Chapman, BarbaraM.},
	Isbn = {978-3-642-02284-5},
	Pages = {154-167},
	Publisher = {Springer Berlin Heidelberg},
	Series = {Lecture Notes in Computer Science},
	Title = {A Proposal to Extend the OpenMP Tasking Model for Heterogeneous Architectures},
	Url = {http://dx.doi.org/10.1007/978-3-642-02303-3_13},
	Volume = {5568},
	Year = {2009},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/978-3-642-02303-3_13}}

@inproceedings{harmony,
	Acmid = {1383447},
	Address = {New York, NY, USA},
	Author = {Diamos, Gregory F. and Yalamanchili, Sudhakar},
	Booktitle = {Proceedings of the 17th international symposium on High performance distributed computing},
	Date-Added = {2013-09-12 14:40:18 +0000},
	Date-Modified = {2013-09-12 14:40:18 +0000},
	Doi = {10.1145/1383422.1383447},
	Isbn = {978-1-59593-997-5},
	Keywords = {dependency graph, gpgpu, harmony, heterogeneous, many core, optimization, performance monitoring, runtime, scheduling},
	Location = {Boston, MA, USA},
	Numpages = {4},
	Pages = {197--200},
	Publisher = {ACM},
	Series = {HPDC '08},
	Title = {Harmony: an execution model and runtime for heterogeneous many core systems},
	Url = {http://doi.acm.org/10.1145/1383422.1383447},
	Year = {2008},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1383422.1383447},
	Bdsk-Url-2 = {http://dx.doi.org/10.1145/1383422.1383447}}

@inproceedings{Diamos:2008fj,
	Acmid = {1383447},
	Address = {New York, NY, USA},
	Author = {Diamos, Gregory F. and Yalamanchili, Sudhakar},
	Booktitle = {Proceedings of the 17th international symposium on High performance distributed computing},
	Date-Added = {2013-09-12 14:39:27 +0000},
	Date-Modified = {2013-09-12 14:39:27 +0000},
	Doi = {10.1145/1383422.1383447},
	Isbn = {978-1-59593-997-5},
	Keywords = {dependency graph, gpgpu, harmony, heterogeneous, many core, optimization, performance monitoring, runtime, scheduling},
	Location = {Boston, MA, USA},
	Numpages = {4},
	Pages = {197--200},
	Publisher = {ACM},
	Series = {HPDC '08},
	Title = {Harmony: an execution model and runtime for heterogeneous many core systems},
	Url = {http://doi.acm.org/10.1145/1383422.1383447},
	Year = {2008},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1383422.1383447},
	Bdsk-Url-2 = {http://dx.doi.org/10.1145/1383422.1383447}}

@book{braga,
	Author = {Braga, Antonio and Carvalho, Andre and Ludermir, Teresa},
	Date-Added = {2013-09-12 13:24:34 +0000},
	Date-Modified = {2013-09-12 13:24:34 +0000},
	Pages = {302},
	Publisher = {LTC},
	Title = {{Redes Neurais Artificiais: Teoria e Aplica{\c c}{\~o}es}},
	Year = {2007}}

@conference{ai,
	Author = {Matthew Browne and Saeed Ghidary},
	Booktitle = {The 16th Australian Joint Conference on AI},
	Date-Added = {2013-09-12 13:24:34 +0000},
	Date-Modified = {2013-09-12 13:24:34 +0000},
	Title = {Convolution neural networks for image processing: an application in robot vision},
	Year = {2003}}

@conference{GPGPU,
	Author = {Duane G. Merrill and Andrew S. Grimshaw},
	Booktitle = {PACT '10 Proceedings of the 19th international conference on Parallel architectures and compilation techniques},
	Date-Added = {2013-09-12 13:24:34 +0000},
	Date-Modified = {2013-09-12 13:24:34 +0000},
	Organization = {University of Virginia, Charlottesville, VA, USA},
	Pages = {545-546},
	Title = {Revisiting sorting for GPGPU stream architectures},
	Year = {2010}}

@conference{cudamlp,
	Author = {Honghoon Jang and Anjin Park and Keechul Jung},
	Booktitle = {Digital Image Computing: Techniques and Applications (DICTA)},
	Date-Added = {2013-09-12 13:24:34 +0000},
	Date-Modified = {2013-09-12 13:24:34 +0000},
	Pages = {155-161},
	Title = {Neural Network Implementation Using CUDA and OpenMP Neural Network Implementation Using CUDA and OpenMP Neural Network Implementation Using CUDA and OpenMP},
	Year = {2008}}

@article{eeg2,
	Author = {Deon Garrett and David Peterson and Charles Anderson and Michael Thaut},
	Date-Added = {2013-09-12 13:24:34 +0000},
	Date-Modified = {2013-09-12 13:24:34 +0000},
	Journal = {Neural Systems and Rehabilitation Engineering, IEEE Transactions on},
	Number = {2},
	Pages = {141 - 144},
	Title = {Neural Systems and Rehabilitation Engineering},
	Volume = {11},
	Year = {2003}}

@book{basicEEG,
	Author = {Ernst Niedermeyer and Fernando Lopes da Silva},
	Date-Added = {2013-09-12 13:24:34 +0000},
	Date-Modified = {2013-09-12 13:24:34 +0000},
	Publisher = {Lippincott Williams & Wilkins},
	Title = {Electroencephalography: Basic Principles, Clinical Applications, and Related Fields},
	Year = {2004}}

@article{eeg3,
	Author = {Shiao-Lin Lin and Yi-Jean Tsai and Cheng-Yuan Liou},
	Date-Added = {2013-09-12 13:24:34 +0000},
	Date-Modified = {2013-09-12 13:24:34 +0000},
	Journal = {Medical and Biological Engineering and Computing},
	Number = {4},
	Pages = {421-426},
	Title = {Conscious mental tasks and their EEG signals},
	Volume = {31},
	Year = {1993}}

@article{decode,
	Author = {John Dylan Haynesm},
	Date-Added = {2013-09-12 13:24:34 +0000},
	Date-Modified = {2013-09-12 13:24:34 +0000},
	Journal = {Nature Reviews Neuroscience},
	Month = {July},
	Pages = {523-534},
	Title = {Decoding mental states from brain activity in humans},
	Volume = {7},
	Year = {2006}}

@article{mental,
	Author = {Charles W. Anderson},
	Date-Added = {2013-09-12 13:24:34 +0000},
	Date-Modified = {2013-09-12 13:24:34 +0000},
	Journal = {Scientific Programming},
	Pages = {171-183},
	Publisher = {IOS Press},
	Title = {Determining Mental State from EEG Signals Using Parallel Implementations of Neural Networks},
	Volume = {4},
	Year = {1995}}

@inbook{Emlet1987,
	Author = {Emlet, R B},
	Booktitle = {Echinoderm Studies},
	Chapter = {2},
	Date-Added = {2013-09-12 13:24:34 +0000},
	Date-Modified = {2013-09-12 13:24:34 +0000},
	Editor = {Jangoux, Michel and Lawrence, John M},
	Pages = {55--136},
	Publisher = {A. A. Balkema Publishers},
	Title = {{Echinoderm Larval Ecology from the Egg}},
	Volume = {2},
	Year = {1987}}

@article{gpuNeural,
	Author = {Sand-Jensen, K.},
	Date-Added = {2013-09-12 13:24:34 +0000},
	Date-Modified = {2013-09-12 13:24:34 +0000},
	Journal = {Pattern Recognition},
	Number = {37},
	Pages = {1311 -- 1314},
	Title = {{Gpu implementation of neural networks}},
	Year = {2004}}

@article{montagemeeg,
	Author = {Sand-Jensen, K.},
	Date-Added = {2013-09-12 13:24:34 +0000},
	Date-Modified = {2013-09-12 13:24:34 +0000},
	Journal = {Journal of Clinical Neurophysiology},
	Number = {11},
	Pages = {30 at 60},
	Publisher = {American Electroencephalographic Society},
	Title = {{Guideline seven: a proposal for standard montages to be used in clinical EEG}},
	Year = {1994}}

@conference{nncuda,
	Address = {Dec.},
	Author = {Jang, Honghoon.},
	Booktitle = {Digital Image Computing: Techniques and Applications (DICTA)},
	Date-Added = {2013-09-12 13:24:34 +0000},
	Date-Modified = {2013-09-12 13:24:34 +0000},
	Number = {1-3},
	Pages = {pp.155,161},
	Title = {Neural Network Implementation Using CUDA and OpenMP},
	Year = {2008}}

@book{Rojas,
	Author = {Raul Rojas},
	Date-Added = {2013-09-12 13:24:34 +0000},
	Date-Modified = {2013-09-12 13:24:34 +0000},
	Publisher = {Springer-Verlag},
	Title = {Neural Networks - A Systematic Introduction},
	Year = {1996}}

@book{haykin,
	Author = {Haykin, Simon},
	Date-Added = {2013-09-12 13:24:34 +0000},
	Date-Modified = {2013-09-12 13:24:34 +0000},
	Pages = {898},
	Publisher = {Bookman},
	Title = {{Redes Neurais: Princ{\'\i}pios e Pr{\'a}tica}},
	Year = {2006}}

@book{alpha,
	Author = {G. Buzsaki},
	Date-Added = {2013-09-12 13:24:34 +0000},
	Date-Modified = {2013-09-12 13:24:34 +0000},
	Publisher = {Oxford University Press},
	Title = {Rhythms of the brain},
	Year = {2006}}

@inproceedings{Gropp:1992uq,
	Author = {Gropp, William D},
	Booktitle = {Fifth International Symposium on Domain Decomposition Methods for Partial Differential Equations, Philadelphia, PA},
	Date-Added = {2013-06-13 01:11:33 +0000},
	Date-Modified = {2013-06-13 01:11:33 +0000},
	Title = {Parallel computing and domain decomposition},
	Year = {1992}}

@inproceedings{pilla2012hierarchical,
	Author = {Pilla, La{\'e}rcio L and Ribeiro, Christiane Pousa and Cordeiro, Daniel and Mei, Chao and Bhatele, Abhinav and Navaux, Philippe OA and Broquedis, Fran{\c{c}}ois and M{\'e}haut, Jean-Fran{\c{c}}ois and Kale, Laxmikant V},
	Booktitle = {Parallel Processing (ICPP), 2012 41st International Conference on},
	Organization = {IEEE},
	Pages = {118--127},
	Title = {A Hierarchical Approach for Load Balancing on Parallel Multi-core Systems},
	Year = 2012}

@article{graham66,
	Author = {Graham, Ronald L.},
	Journal = {Bell System Technical Journal},
	Month = nov,
	Number = 9,
	Pages = {1563--1581},
	Title = {Bounds for certain multiprocessing anomalies},
	Volume = 45,
	Year = 1966}

@book{handbook-sched,
	Address = {Boca Raton, FL, USA},
	Edition = 1,
	Editor = {Leung, Joseph Y. T.},
	Isbn = {1-58488-397-9},
	Publisher = {Chapman \& Hall/CRC Press},
	Series = {Computer and Information Science Series},
	Title = {Handbook of Scheduling: Algorithms, Models, and Performance Analysis},
	Year = 2004}

@book{GaJo1979,
	Author = {Garey, Michael R. and Johnson, David S.},
	Isbn = {0-716-71045-5},
	Month = jan,
	Publisher = {W. H. Freeman and Company},
	Title = {Computers and Intractability: A Guide to the Theory of NP-Completeness},
	Year = 1979}

@article{Aprendizado,
	Author = {Qi Li and Raied Salman and Erik Test and Robert Strack and Vojislav Kecman},
	Date-Added = {2013-04-28 18:34:20 +0000},
	Date-Modified = {2013-04-28 18:34:20 +0000},
	Journal = {Central European Journal of Computer Science},
	Month = {December},
	Number = {4},
	Pages = {387-405},
	Title = {GPUSVM: a comprehensive CUDA based support vector machine package},
	Volume = {1},
	Year = {2011}}

@conference{visualizacao,
	Author = {Eduardo Camargo and Segio Kostin and Raquel Pinto},
	Booktitle = {Sistemas Computacionais (WSCAD-SSC), 2011 Simposio em},
	Date-Added = {2013-04-28 18:34:11 +0000},
	Date-Modified = {2013-04-28 18:34:11 +0000},
	Pages = {10-10},
	Title = {A Tool for Scientific Visualization Based on Particle Tracing Algorithm on Graphics Processing Units},
	Year = {2011}}

@article{fluido,
	Author = {E. Riegel and T. Indinger and N. A. Adams},
	Date-Added = {2013-04-28 18:33:57 +0000},
	Date-Modified = {2013-04-28 18:33:57 +0000},
	Journal = {Computer Science - Research and Development},
	Number = {3-4},
	Pages = {241-247},
	Title = {Implementation of a Lattice--Boltzmann method for numerical fluid mechanics using the nVIDIA CUDA technology},
	Volume = {23},
	Year = {2009}}

@misc{cuda,
	Address = {Santa Clara CA, EUA},
	Annote = {\{ISBN\}DU-05347-001_v03 },
	Author = {{CUDA Development Core Team}},
	Date-Added = {2013-04-28 18:33:35 +0000},
	Date-Modified = {2013-04-28 18:33:35 +0000},
	Keywords = {CUDA},
	Title = {{CUDA 4.1 Programming Guide}},
	Url = {http://www.nvidia.com/cuda},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nvidia.com/cuda}}

@article{gpu,
	Author = {D. Owens and M. Houston and D. Luebke and S. Green and J. E. Stone and J. C. Phillips},
	Journal = {Proceedings of the IEEE},
	Keywords = {gpus},
	Number = 1,
	Pages = {879--899},
	Title = {{GPU} computing},
	Volume = 96,
	Year = 2008}

@article{merge,
	Author = {Linderman, Michael D. and Collins, Jamison D. and Wang, Hong and Meng, Teresa H.},
	Date-Added = {2013-04-17 13:17:39 +0000},
	Date-Modified = {2013-04-17 13:18:26 +0000},
	Journal = {SIGPLAN Not.},
	Keywords = {GPGPU, heterogeneous multi-core, predicate dispatch},
	Month = {mar},
	Number = {3},
	Pages = {287--296},
	Title = {Merge: a programming model for heterogeneous multi-core systems},
	Volume = {43},
	Year = {2008}}

@inproceedings{tasks,
	Address = {Raleigh, NC},
	Author = {Marisabel Guevara and Chris Gregg and Kim Hazelwood and Kevin Skadron},
	Booktitle = {Workshop on Programming Models for Emerging Architectures},
	Date-Added = {2013-04-17 13:11:12 +0000},
	Date-Modified = {2013-04-17 13:12:35 +0000},
	Month = {September},
	Pages = {69--76},
	Series = {PMEA},
	Title = {Enabling Task Parallelism in the CUDA Scheduler},
	Year = {2009}}

@inproceedings{graphics,
	Address = {Aire-la-Ville, Switzerland, Switzerland},
	Author = {Cederman, Daniel and Tsigas, Philippas},
	Booktitle = {Proceedings of the 23rd ACM SIGGRAPH/EUROGRAPHICS symposium on Graphics hardware},
	Date-Added = {2013-04-17 13:04:33 +0000},
	Date-Modified = {2013-04-17 13:06:01 +0000},
	Pages = {57--64},
	Publisher = {Eurographics Association},
	Series = {GH '08},
	Title = {On dynamic load balancing on graphics processors},
	Year = {2008}}

@article{ray,
	Author = {M. Mller, C. and Strengert, M. and Ertl, T.},
	Date-Added = {2013-04-17 13:01:39 +0000},
	Date-Modified = {2013-04-17 13:03:09 +0000},
	Journal = {Parallel Comput.},
	Keywords = {Distributed applications, Distributed/network graphics, Viewing algorithms},
	Month = {Jun},
	Number = {6},
	Pages = {406--419},
	Title = {Adaptive load balancing for raycasting of non-uniformly bricked volumes},
	Volume = {33},
	Year = {2007}}

@inproceedings{kdtree,
	Address = {New York, NY, USA},
	Author = {Foley, Tim and Sugerman, Jeremy},
	Booktitle = {Proceedings of the ACM SIGGRAPH/EUROGRAPHICS conference on Graphics hardware},
	Date-Added = {2013-04-17 12:59:08 +0000},
	Date-Modified = {2013-04-17 13:00:35 +0000},
	Keywords = {KDTREE},
	Pages = {15--22},
	Publisher = {ACM},
	Title = {KD-tree acceleration structures for a GPU raytracer},
	Year = {2005}}

@inproceedings{siang,
	Author = {de Camargo, R.Y.},
	Booktitle = {High Performance Computing (HiPC), 2011 18th International Conference on},
	Date-Added = {2013-04-09 18:52:43 +0000},
	Date-Modified = {2013-04-09 18:53:43 +0000},
	Keywords = {digital simulation;graphics processing units;neural nets;parallel architectures;CUDA algorithm;biological system simulations;graphic boards;graphical processing units;multiGPU algorithm;neuronal network simulation communication;physical system simulations;quadcore CPU;Biological neural networks;Computational modeling;Graphics processing unit;Instruction sets;Kernel;Mathematical model;Neurons},
	Pages = {1-10},
	Title = {A multi-GPU algorithm for communication in neuronal network simulations},
	Year = {2011}}

@inproceedings{rozante,
	Author = {Borelli, F.F. and Camargo, R.Y. and Martins, D.C. and Stransky, B. and Rozante, L.C.S.},
	Booktitle = {Computational Advances in Bio and Medical Sciences (ICCABS), 2012 IEEE 2nd International Conference on},
	Date-Added = {2013-04-09 18:50:11 +0000},
	Date-Modified = {2013-04-09 18:50:55 +0000},
	Keywords = {bioinformatics;computational complexity;feature extraction;genetics;graphics processing units;inference mechanisms;parallel architectures;parallel programming;GPU architectures;GPU-CUDA programming;GRN inference;bioinformatics;computational complexity;exhaustive search;feature selection method;feature subset;gene expression data;gene interactions;gene regulatory network inference;parallel algorithm;parallel architecture;parallel programming model;search algorithm;Computer architecture;Entropy;Graphics processing unit;Instruction sets;Performance evaluation;Prediction algorithms;Random access memory;CUDA;GPU Computing;GRNs Inference;exhaustive search;mean conditional entropy},
	Pages = {1-6},
	Title = {Accelerating gene regulatory networks inference through GPU/CUDA programming},
	Year = {2012}}

@inproceedings{snucl,
	Address = {New York, NY, USA},
	Author = {Kim, Jungwon and Seo, Sangmin and Lee, Jun and Nah, Jeongho and Jo, Gangwon and Lee, Jaejin},
	Booktitle = {Proceedings of the 26th ACM international conference on Supercomputing},
	Date-Added = {2013-04-09 18:30:14 +0000},
	Date-Modified = {2013-04-09 18:31:30 +0000},
	Keywords = {clusters, heterogeneous computing, opencl, programming models},
	Pages = {341--352},
	Publisher = {ACM},
	Series = {ICS '12},
	Title = {SnuCL: an OpenCL framework for heterogeneous CPU/GPU clusters},
	Year = {2012}}

@inproceedings{wave,
	Author = {Long Wang and Weile Jia and Xuebin Chi and Yue Wu and Weiguo Gao and Lin-Wang Wang},
	Booktitle = {High Performance Computing, Networking, Storage and Analysis (SC), 2011 International Conference for},
	Date-Added = {2013-04-09 18:27:23 +0000},
	Date-Modified = {2013-04-09 18:28:25 +0000},
	Keywords = {chemistry computing;density functional theory;graphics processing units;multiprocessing systems;pseudopotential methods;CPU DFT-PWP code;CPU computing units;GPU clusters;GPU computing units;PEtot;band-index parallelization;hybrid reciprocal-space parallelization;large scale plane wave pseudopotential density functional theory calculation;Discrete Fourier transforms;Graphics processing unit;Indexes;Mathematical model;Vectors;Wave functions;Density functional theory;Electronic structure;First-principles;GPU;Plane wave pseudopotential},
	Pages = {1-10},
	Title = {Large scale plane wave pseudopotential density functional theory calculations on GPU clusters},
	Year = {2011}}

@inproceedings{appCientificas,
	Address = {New York, NY, USA},
	Author = {Wang, Lingyuan and Huang, Miaoqing and Narayana, Vikram K. and El-Ghazawi, Tarek},
	Booktitle = {Proceedings of the 8th ACM International Conference on Computing Frontiers},
	Date-Added = {2013-04-09 18:10:37 +0000},
	Date-Modified = {2013-04-09 18:12:38 +0000},
	Keywords = {GPU, UPC, hybrid parallel programming, multicore},
	Pages = {6:1--6:10},
	Publisher = {ACM},
	Series = {CF '11},
	Title = {Scaling scientific applications on clusters of hybrid multicore/GPU nodes},
	Year = {2011}}

@inproceedings{cluster,
	Address = {Washington, DC, USA},
	Author = {Fan, Zhe and Qiu, Feng and Kaufman, Arie and Yoakum-Stover, Suzanne},
	Booktitle = {Proceedings of the 2004 ACM/IEEE conference on Supercomputing},
	Date-Added = {2013-04-09 17:59:08 +0000},
	Date-Modified = {2013-04-09 18:01:56 +0000},
	Keywords = {GPU cluster, data intensive computing, lattice Boltzmann model, urban airborne dispersion, computational fluid dynamics},
	Pages = {47--},
	Publisher = {IEEE Computer Society},
	Series = {SC '04},
	Title = {GPU Cluster for High Performance Computing},
	Year = {2004}}

@manual{nvidiaguide,
	Author = {Nvidia},
	Date-Added = {2013-03-27 11:56:29 +0000},
	Date-Modified = {2013-03-27 11:57:59 +0000},
	Organization = {NVIDIA CUDA},
	Title = {Programming Guide 2.3},
	Year = {2009}}

@url{websitecuda,
	Author = {Nvidia},
	Date-Added = {2013-03-27 11:51:49 +0000},
	Date-Modified = {2013-03-27 12:02:57 +0000},
	Keywords = {CUDA, NVIDIA},
	Lastchecked = {23 march 2013},
	Month = {Mar},
	Title = {http://www.nvidia.com},
	Url = {http://www.nvidia.com},
	Urldate = {http://www.nvidia.com},
	Year = {2013},
	Bdsk-Url-1 = {http://www.nvidia.com}}

@article{Nickolls,
	Abstract = {The advent of multicore CPUs and manycore GPUs means that mainstream processor chips are now parallel systems. Furthermore, their parallelism continues to scale with Moore's law. The challenge is to develop mainstream application software that transparently scales its parallelism to leverage the increasing number of processor cores, much as 3D graphics applications transparently scale their parallelism to manycore GPUs with widely varying numbers of cores.},
	Author = {Nickolls, John and Buck, Ian and Garland, Michael and Skadron, Kevin},
	Date-Added = {2013-03-27 11:46:32 +0000},
	Date-Modified = {2013-03-27 11:48:18 +0000},
	Journal = {Queue},
	Month = {mar},
	Number = {2},
	Pages = {40--53},
	Title = {Scalable Parallel Programming with CUDA},
	Volume = {6},
	Year = {2008}}

@conference{Harish,
	Abstract = {Large graphs involving millions of vertices are common in many practical applications and are challenging to process. Practical-time implementations using high-end computers are reported but are accessible only to a few. Graphics Processing Units (GPUs) of today have high computation power and low price. They have a restrictive programming model and are tricky to use. The G80 line of Nvidia GPUs can be treated as a SIMD processor array using the CUDA programming model. We present a few fundamental algorithms - including breadth first search, single source shortest path, and all-pairs shortest path - using CUDA on large graphs. We can compute the single source shortest path on a 10 million vertex graph in 1.5 seconds using the Nvidia 8800GTX GPU costing $600. In some cases optimal sequential algorithm is not the fastest on the GPU architecture. GPUs have great potential as high-performance co-processors.},
	Address = {Berlin, Heidelberg},
	Author = {Harish, Pawan and Narayanan, P. J.},
	Booktitle = {Proceedings of the 14th international conference on High performance computing},
	Date-Added = {2013-03-27 11:42:02 +0000},
	Date-Modified = {2013-03-27 11:43:50 +0000},
	Pages = {197--208},
	Publisher = {Springer-Verlag},
	Series = {HiPC'07},
	Title = {Accelerating large graph algorithms on the GPU using CUDA},
	Year = {2007}}

@conference{Boyer,
	Abstract = {The availability of easily programmable manycore CPUs and GPUs has motivated investigations into how to best exploit their tremendous computational power for scientific computing. Here we demonstrate how a systems biology application---detection and tracking of white blood cells in video microscopy---can be accelerated by 200× using a CUDA-capable GPU. Because the algorithms and implementation challenges are common to a wide range of applications, we discuss general techniques that allow programmers to make efficient use of a manycore GPU.},
	Address = {Washington, DC, USA},
	Author = {Boyer, Michael and Tarjan, David and Acton, Scott T. and Skadron, Kevin},
	Booktitle = {Proceedings of the 2009 IEEE International Symposium on Parallel\&Distributed Processing},
	Date-Added = {2013-03-27 11:38:19 +0000},
	Date-Modified = {2013-03-27 11:40:37 +0000},
	Pages = {1--12},
	Publisher = {IEEE Computer Society},
	Series = {IPDPS '09},
	Title = {Accelerating leukocyte tracking using CUDA: A case study in leveraging manycore coprocessors},
	Year = {2009}}

@conference{van,
	Abstract = {The research area of Multimedia Content Analysis (MMCA) considers all aspects of the automated extraction of knowledge from multimedia archives and data streams. To satisfy the increasing computational demands of MMCA problems, the use of High Performance Computing (HPC) techniques is essential. As most MMCA researchers are not HPC experts, there is an urgent need for `familiar' programming models and tools that are both easy to use and efficient.

Today, several user transparent library-based parallelization tools exist that aim to satisfy both these requirements. In general, such tools focus on data parallel execution on traditional compute clusters. As of yet, none of these tools also incorporate the use of many-core processors (e.g. GPUs), however. While traditional clusters are now being transformed into GPU-clusters, programming complexity vastly increases -- and the need for easy and efficient programming models is as urgent as ever.

This paper presents our first steps in the direction of obtaining a user transparent programming model for data parallel and hierarchical multimedia computing on GPU-clusters. The model is obtained by extending an existing user transparent parallel programming system (applicable to traditional compute clusters) with a set of CUDA compute kernels. We show our model to be capable of obtaining orders-of-magnitude speed improvements, without requiring any additional effort from the application programmer.},
	Address = {Berlin, Heidelberg},
	Author = {van Werkhoven, Ben and Maassen, Jason and Seinstra, Frank J.},
	Booktitle = {Proceedings of the 2010 international conference on Computer Architecture},
	Date-Added = {2013-03-27 00:47:31 +0000},
	Date-Modified = {2013-03-27 00:49:27 +0000},
	Pages = {28--39},
	Publisher = {Springer-Verlag},
	Series = {ISCA'10},
	Title = {Towards user transparent parallel multimedia computing on GPU-Clusters},
	Year = {2012}}

@conference{Kim,
	Abstract = {In this paper, we propose SnuCL, an OpenCL framework for heterogeneous CPU/GPU clusters. We show that the original OpenCL semantics naturally fits to the heterogeneous cluster programming environment, and the framework achieves high performance and ease of programming. The target cluster architecture consists of a designated, single host node and many compute nodes. They are connected by an interconnection network, such as Gigabit Ethernet and InfiniBand switches. Each compute node is equipped with multicore CPUs and multiple GPUs. A set of CPU cores or each GPU becomes an OpenCL compute device. The host node executes the host program in an OpenCL application. SnuCL provides a system image running a single operating system instance for heterogeneous CPU/GPU clusters to the user. It allows the application to utilize compute devices in a compute node as if they were in the host node. No communication API, such as the MPI library, is required in the application source. SnuCL also provides collective communication extensions to OpenCL to facilitate manipulating memory objects. With SnuCL, an OpenCL application becomes portable not only between heterogeneous devices in a single node, but also between compute devices in the cluster environment. We implement SnuCL and evaluate its performance using eleven OpenCL benchmark applications.},
	Address = {New York, NY, USA},
	Author = {Kim, Jungwon and Seo, Sangmin and Lee, Jun and Nah, Jeongho and Jo, Gangwon and Lee, Jaejin},
	Booktitle = {Proceedings of the 26th ACM international conference on Supercomputing},
	Date-Added = {2013-03-27 00:45:17 +0000},
	Date-Modified = {2013-03-27 00:47:13 +0000},
	Keywords = {clusters, heterogeneous computing, opencl, programming models},
	Pages = {341--352},
	Publisher = {ACM},
	Series = {ICS '12},
	Title = {SnuCL: an OpenCL framework for heterogeneous CPU/GPU clusters},
	Year = {2012}}

@conference{Flat,
	Abstract = {For leveraging multiple GPUs in a cluster system, it is necessary to assign application tasks to multiple GPUs and execute those tasks with appropriately using communication primitives to handle data transfer among GPUs. In current GPU programming models, communication primitives such as MPI functions cannot be used within GPU kernels. Instead, such functions should be used in the CPU code. Therefore, programmer must handle both GPU kernel and CPU code for data communications. This makes GPU programming and its optimization very difficult.

In this paper, we propose a programming framework named FLAT which enables programmers to use MPI functions within GPU kernels. Our framework automatically transforms MPI functions written in a GPU kernel into runtime routines executed on the CPU. The execution model and the implementation of FLAT are described, and the applicability of FLAT in terms of scalability and programmability is discussed. We also evaluate the performance of FLAT. The result shows that FLAT achieves good scalability for intended applications.},
	Address = {New York, NY, USA},
	Author = {Miyoshi, Takefumi and Irie, Hidetsugu and Shima, Keigo and Honda, Hiroki and Kondo, Masaaki and Yoshinaga, Tsutomu},
	Booktitle = {Proceedings of the 5th Annual Workshop on General Purpose Processing with Graphics Processing Units},
	Date-Added = {2013-03-27 00:41:41 +0000},
	Date-Modified = {2013-04-09 18:33:40 +0000},
	Keywords = {GPGPU, MPI, cluster system, programming framework},
	Pages = {20--29},
	Publisher = {ACM},
	Series = {GPGPU-5},
	Title = {FLAT: a GPU programming framework to provide embedded MPI},
	Year = {2012}}

@conference{long,
	Abstract = {In this work, we present our implementation of the density functional theory (DFT) plane wave pseudopotential (PWP) calculations on GPU clusters. This GPU version is developed based on a CPU DFT-PWP code: PEtot, which can calculate up to a thousand atoms on thousands of processors. Our test indicates that the GPU version can have a ~10 times speed-up over the CPU version. A detail analysis of the speed-up and the scaling on the number of CPU/GPU computing units (up to 256) are presented. The success of our speed-up relies on the adoption a hybrid reciprocal-space and band-index parallelization scheme. As far as we know, this is the first GPU DFT-PWP code scalable to large number of CPU/GPU computing units. We also outlined the future work, and what is needed to further increase the computational speed by another factor of 10.},
	Address = {New York, NY, USA},
	Author = {Wang, Long and Wu, Yue and Jia, Weile and Gao, Weiguo and Chi, Xuebin and Wang, Lin-Wang},
	Booktitle = {Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and Analysis},
	Date-Added = {2013-03-27 00:38:15 +0000},
	Date-Modified = {2013-03-27 00:40:43 +0000},
	Keywords = {GPU, density functional theory, electronic structure, first-principles, plane wave pseudopotential},
	Pages = {71:1--71:10},
	Publisher = {ACM},
	Series = {SC '11},
	Title = {Large scale plane wave pseudopotential density functional theory calculations on GPU clusters},
	Year = {2011}}

@conference{wang,
	Abstract = {Rapid advances in the performance and programmability of graphics accelerators have made GPU computing a compelling solution for a wide variety of application domains. However, the increased complexity as a result of architectural heterogeneity and imbalances in hardware resources poses significant programming challenges in harnessing the performance advantages of GPU accelerated parallel systems. Moreover, the speedup derived from GPU often gets offset by longer communication latencies and inefficient task scheduling. To achieve the best possible performance, a suitable parallel programming model is therefore essential.

In this paper, we explore a new hybrid parallel programming model that incorporates GPU acceleration with the Partitioned Global Address Space (PGAS) programming paradigm. As we demonstrate, by combining Unified Parallel C (UPC) and CUDA as a case study, this hybrid model offers programmers with both enhanced programmability and powerful heterogeneous execution. Two application benchmarks, namely NAS Parallel Benchmark (NPB) FT and MG, are used to show the effectiveness of our proposed hybrid approach. Experimental results indicate that both implementations achieve significantly better performance due to optimization opportunities offered by the hybrid model, such as the funneled execution mode and fine-grained overlapping of communication and computation.},
	Address = {New York, NY, USA},
	Author = {Wang, Lingyuan and Huang, Miaoqing and Narayana, Vikram K. and El-Ghazawi, Tarek},
	Booktitle = {Proceedings of the 8th ACM International Conference on Computing Frontiers},
	Date-Added = {2013-03-27 00:35:40 +0000},
	Date-Modified = {2013-03-27 00:38:08 +0000},
	Keywords = {GPU, UPC, hybrid parallel programming, multicore},
	Pages = {6:1--6:10},
	Publisher = {ACM},
	Series = {CF '11},
	Title = {Scaling scientific applications on clusters of hybrid multicore/GPU nodes},
	Year = {2011}}

@conference{acosta,
	Abstract = {The advent of multicore systems, joined to the potential acceleration of the graphics processing units, alleviates some well known important architectural problems at the expense of a considerable increment of the programmability wall. The heterogeneity, both at architectural and programming level at the same time, raises the programming difficulties. Adapting existing code and libraries to such systems is a fundamental problem. The performance of this code is affected by the large interdependence between the code and the parallel architecture. We have developed a dynamic load balancing library that allows parallel code to be adapted to a wide variety of heterogeneous systems. The overhead introduced by our system is minimal and the cost to the programmer negligible. This system has been applied to solve load imbalance problems appearing in homogeneous and heterogeneous multi-GPU platforms. As case studies, we consider matrix multiply and resource allocation problems, in different heterogeneous scenarios in multi-GPU systems. The unbalanced nature of these algorithms and heterogeneous systems allowed us to test the success of our load balancing approach.},
	Author = {Acosta, A. and Blanco, V. and Almeida, F.},
	Booktitle = {Parallel and Distributed Processing with Applications (ISPA), 2012 IEEE 10th International Symposium on},
	Date-Added = {2013-03-27 00:33:20 +0000},
	Date-Modified = {2013-03-27 00:34:49 +0000},
	Keywords = {dynamic programming;graphics processing units;matrix multiplication;multiprocessing systems;parallel architectures;parallel programming;resource allocation;software architecture;architectural level heterogeneity;architectural problems;dynamic load balancing library;graphics processing units;heterogeneous multiGPU platform;heterogeneous multiGPU systems;homogeneous multiGPU platform;load imbalance problems;matrix multiplication problems;multicore systems;parallel architecture;parallel code;programming level heterogeneity;resource allocation problems;Graphics processing unit;Heuristic algorithms;Instruction sets;Libraries;Load management;Resource management;CUDA;Dynamic Load Balancing;Dynamic Programming;GPU;Irregular code;Iterative algorithms;Resource Allocation Problem},
	Pages = {646-653},
	Title = {Towards the Dynamic Load Balancing on Heterogeneous Multi-GPU Systems.},
	Year = {2012}}

@conference{raphael,
	Abstract = {Clusters of GPUs are becoming commonly used to execute computationally demanding applications. Due to the frequent changes in GPU architecture, many clusters contain heterogeneous types of GPUs, leading to the problem of load distribution among the machines. In this work, we propose a load distribution algorithm for scientific applications executed in heterogeneous GPU clusters. The algorithm finds a distribution of data that minimizes the execution time of the application, by guaranteeing that all GPUs spend the same amount of time processing its assigned kernels and data. We use the algorithm to execute the simulation of large scale neuronal networks. We show that the algorithm effectively balances the load among the GPUs and reduces the execution time of the application.},
	Author = {de Camargo, R.Y.},
	Booktitle = {Applications for Multi-Core Architectures (WAMCA), 2012 Third Workshop on},
	Date-Added = {2013-03-27 00:28:35 +0000},
	Date-Modified = {2013-03-27 00:31:54 +0000},
	Keywords = {graphics processing units;neural nets;GPU architecture;data distribution;heterogeneous GPU clusters;load distribution algorithm;neuronal networks;Biological neural networks;Clustering algorithms;Computational modeling;Graphics processing units;Kernel;Mathematical model;Neurons;CUDA;GPGPU;GPU cluster;load-balancing},
	Pages = {1-6},
	Title = {A Load Distribution Algorithm Based on Profiling for Heterogeneous GPU Clusters},
	Year = {2012}}

@conference{dynamicIsmael,
	Abstract = {Parallel computing in heterogeneous environments is drawing considerable attention due to the growing number of these kind of systems. Adapting existing code and libraries to such systems is a fundamental problem. The performance of this code is affected by the large interdependence between the code and these parallel architectures. We have developed a dynamic load balancing library that allows parallel code to be adapted to heterogeneous systems for a wide variety of problems. The overhead introduced by our system is minimal and the cost to the programmer negligible. The strategy was validated on several problems to confirm the soundness of our proposal.},
	Address = {Berlin, Heidelberg},
	Author = {Galindo, Ismael and Almeida, Francisco and Bad{\'\i}a-Contelles, Jos{\'e} Manuel},
	Booktitle = {Proceedings of the 15th European PVM/MPI Users' Group Meeting on Recent Advances in Parallel Virtual Machine and Message Passing Interface},
	Date-Added = {2013-03-27 00:22:17 +0000},
	Date-Modified = {2013-03-27 00:26:52 +0000},
	Number = {11},
	Pages = {64--74},
	Publisher = {Springer-Verlag},
	Title = {Dynamic Load Balancing on Dedicated Heterogeneous Systems},
	Year = {2008}}

@conference{dynamicLoad,
	Abstract = {The computational power provided by many-core graphics processing units (GPUs) has been exploited in many applications. The programming techniques currently employed on these GPUs are not sufficient to address problems exhibiting irregular, and unbalanced workload. The problem is exacerbated when trying to effectively exploit multiple GPUs concurrently, which are commonly available in many modern systems. In this paper, we propose a task-based dynamic load-balancing solution for single-and multi-GPU systems. The solution allows load balancing at a finer granularity than what is supported in current GPU programming APIs, such as NVIDIA's CUDA. We evaluate our approach using both micro-benchmarks and a molecular dynamics application that exhibits significant load imbalance. Experimental results with a single-GPU configuration show that our fine-grained task solution can utilize the hardware more efficiently than the CUDA scheduler for unbalanced workload. On multi-GPU systems, our solution achieves near-linear speedup, load balance, and significant performance improvement over techniques based on standard CUDA APIs.},
	Author = {Long Chen and Villa, O. and Krishnamoorthy, S. and Gao, G.R.},
	Booktitle = {Parallel Distributed Processing (IPDPS), 2010 IEEE International Symposium on},
	Date-Added = {2013-03-27 00:15:28 +0000},
	Date-Modified = {2013-03-27 00:20:53 +0000},
	Keywords = {computer graphic equipment;coprocessors;resource allocation;CUDA scheduler;NVIDIA;fine-grained task solution;graphics processing units;microbenchmarks;molecular dynamics application;multiGPU systems;single GPU systems;task-based dynamic load-balancing solution;Concurrent computing;Hardware;High performance computing;Laboratories;Load management;Parallel processing;Parallel programming;Power engineering and energy;Power engineering computing;Processor scheduling},
	Pages = {1-12},
	Title = {Dynamic load balancing on single- and multi-GPU systems},
	Year = {2010}}

@techreport{starpu,
	Abstract = {Multicore machines equipped with accelerators are becoming increasingly popular. The TOP500-leading RoadRunner machine is probably the most famous example of a parallel computer mixing IBM Cell Broadband Engines and AMD opteron processors. Other architectures, featuring GPU accelerators, are expected to appear in the near future. To fully tap into the potential of these hybrid machines, pure offloading approaches, in which the main core of the application runs on regular processors and offloads specific parts on accelerators, are not sufficient. The real challenge is to build systems where the application would permanently spread across the entire machine, that is, where parallel tasks would be dynamically scheduled over the full set of available processing units. To face this challenge, we propose a new runtime system capable of scheduling tasks over heterogeneous, accelerator-based machines. Our system features a software virtual shared memory that provides a weak consistency model. The system keeps track of data copies within accelerator embedded-memories and features a data-prefetching engine. Such facilities, together with a database of self-tuned per-task performance models, can be used to greatly improve the quality of scheduling policies in this context. We demonstrate the relevance of our approach by benchmarking various parallel numerical kernel implementations over our runtime system. We obtain significant speedups and a very high efficiency on various typical workloads over multicore machines equipped with multiple accelerators.},
	Address = {http://hal.inria.fr/inria-00467677/PDF/RR-7240.pdf},
	Annote = {Documento Franc{\^e}s, n{\~a}o sei exatamente o que {\'e} Documento de pesquisa},
	Author = {C{\'e}dric Augonnet and Samuel Thibault and Raymond Namyst},
	Date-Added = {2013-03-27 00:03:01 +0000},
	Date-Modified = {2013-03-27 00:12:04 +0000},
	Institution = {Laboratoire Bordelais de Recherche en Informatique - LaBRI, RUNTIME - INRIA Bordeaux - Sud-Ouest},
	Keywords = {GPGPU, Multicore, Scheduling, Performance, StarPU},
	Month = {Mar},
	Number = {RR-7240},
	Title = {StarPU: a Runtime System for Scheduling Tasks over Accelerator-Based Multicore Machines},
	Type = {Rapport de recherche},
	Year = {2010}}
